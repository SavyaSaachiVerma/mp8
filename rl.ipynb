{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE 534 HW: Reinforcement Learning\n",
    "    v1, Designed for IE 534/CS 547 Deep Learning, Fall 2019 at UIUC\n",
    "\n",
    "In this assignment, we will experiment with the (deep) reinforcement learning algorithms covered in the lecture. In particular, you will implement variants of the popular `DQN` (Deep Q-Network) (1) and `A2C` (Advantage Actor-Critic) (2) algorithms (by the same first author! orz), and test your implementation on both a small example (CartPole problem) and an Atari game (Breakout game). We focus on model-free algorithms rather than model-based ones, because neural nets are easier applicable and more popular nowadays in the model-free setting. (When the system dynamic is known or can be easily inferred, model-based can sometimes do better.)\n",
    "\n",
    "The assignment breaks into **three parts**:\n",
    "\n",
    "- **In Part I** (50 pts), you basically need to follow the instructions in this notebook to do a little bit of coding. We'll be able to see if your code trains by testing against the CartPole environment provided by the OpenAI gym package. We'll generate some plots that are required for grading.\n",
    "\n",
    "- **In Part II** (40 pts), you'll copy your code onto Blue Waters (or actually any good server..), and run a much larger-scale experiment with the Breakout game. Hopefully, you can teach the computer to play Breakout in less than half a day! Share your final game score in this notebook. **<font color=red>This part will take at least a day. Please start early!!</font>**\n",
    "\n",
    "- **In Part III** (10 pts), you'll be asked to think about a few questions. These questions are mostly open-ended. Please write down your thoughts on them.\n",
    "\n",
    "Finally, after you finished everything in this notebook **<font color=red>(code snippets C1-C5, plots P1-P5, question answers Q1-Q5)</font>**, please save the notebook, and export to a PDF (or an HTML file), and submit:\n",
    "    \n",
    "1. the **.ipynb notebook and exported .pdf/.html file**, PDF is preferred (I usually do File -> Print Preview -> use Chrome's Save as PDF);\n",
    "\n",
    "2. your code (**Algo.py, Model.py files**);\n",
    "\n",
    "3. job artifacts (**.log files** only, pytorch models and images not required)\n",
    "\n",
    "to Compass 2g for grading.\n",
    "\n",
    "**PS: Remember to save your notebook occasionally as you work through it!**\n",
    "\n",
    "#### References\n",
    "\n",
    "- (1) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. Nature, 518(7540), p.529.\n",
    "- (2) Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In International conference on machine learning (pp. 1928-1937).\n",
    "- (3) A useful tutorial: https://spinningup.openai.com/en/latest/\n",
    "- (4) *Useful code references*: https://github.com/deepmind/bsuite; https://github.com/openai/baselines; https://github.com/astooke/rlpyt;\n",
    "\n",
    "***\n",
    "First of all, **enter your NetID here** in the cell below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your NetID: ssverma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: DQN and A2C on CartPole\n",
    "***\n",
    "This part is designed to run on your own local laptop/PC.\n",
    "\n",
    "Before you start, there are some python dependencies: `pytorch, gym, numpy, multiprocessing, matplotlib`. Please install them correctly. You can install `pytorch` following instruction here https://pytorch.org/get-started/locally/. The code is compatible with PyTorch 0.4.x ~ 1.x. PyTorch 1.1 with cuda 10.0 worked for me (`conda install pytorch==1.1.0 torchvision==0.3.0 cudatoolkit=10.0 -c pytorch`).\n",
    "\n",
    "Please <font color=red>**always**</font> run the code cell below each time you open this notebook, to make sure `gym` is installed and to enable `autoreload` which **allows code changes to be effective immediately**. So if you changed `Algo.py` or `Model.py` but the test codes are not reflecting your changes, restart the notebook kernel and run this cell!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.17.2)\n",
      "Requirement already satisfied: six in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (4.1.2.30)\n",
      "Requirement already satisfied: future in c:\\users\\12179\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# install openai gym\n",
    "%pip install gym\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Code Structure\n",
    "\n",
    "The code is structured in 5 python files:\n",
    "\n",
    "- `Main.py`: contains the main entry point and training loop\n",
    "- `Model.py`: constructs the torch neural network modules\n",
    "- `Env.py`: contains the environment simulations interface, based on openai gym\n",
    "- `Algo.py`: implements the DQN and A2C algorithms\n",
    "- `Replay.py`: implements the experience replay buffer for DQN\n",
    "- `Draw.py`: saves some game snapshots to jpeg files\n",
    "\n",
    "Some parts of the code `Model.py` and `Algo.py` are left blank for you to complete. You are not required to modify the other parts (unless, of course, you want to boost the performance!). This is kind of a minimalist implementation, and might be different from the other code on the internet in details. You're welcomed to improve it,  after you've finished all the required things of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 OpenAI gym and CartPole environment\n",
    "OpenAI developed python package `gym` a while ago to facilitate RL research. `gym` provides a common interface between the program and the environments. For instance, the code cell below will create the CartPole environment. A window will show up when you run the code. The goal is to keep adjusting the cart so that the pole stays in its upright position.\n",
    "\n",
    "A demo video from OpenAI:\n",
    "<video width=\"320\" controls src=\"http://s3-us-west-2.amazonaws.com/rl-gym-doc/cartpole-no-reset.mp4\" />\n",
    "\n",
    "`gym` also provides interface to Atari games. However, installing package `atari-py` is not easy on Windows/Mac, so we won't demonstrate it here. More info: http://gym.openai.com/docs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "    if done: break\n",
    "    time.sleep(0.15)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Deep Q Learning\n",
    "\n",
    "A little recap on DQN. We learned from lecture that Q-Learning is a model-free reinforcement learning algorithm. It falls into the off-policy type algorithm since it can utilize past experiences stored in a buffer. It also falls into the (approximate) dynamic programming type algorithm, since it tries to learn an optimal state-action value function using time difference (TD) errors. Q Learning is particularly interesting because it exploits the optimality structure in MDP. It's related to the Hamilton–Jacobi–Bellman equation in classical control.\n",
    "\n",
    "For MDP\n",
    "$$\n",
    "M = (S,A,P,r,\\gamma)\n",
    "$$\n",
    "where $S$ is the state space, $A$ is the action space, $P$ is the transition dynamic, $r(s,a)$ is a reward function $S\\times A \\mapsto R$, and $\\gamma$ is the discount factor.\n",
    "\n",
    "The tabular case (when $S,A$ are finite), Q-Learning does the following value iteration update repeatedly when collecting experience $(s_t, a_t, r_t)$ ($\\eta$ is the learning rate):\n",
    "$$\n",
    "Q^{new}(s_t, a_t) \\leftarrow Q^{old}(s_t, a_t) + \\eta \\left( r_t + \\gamma \\max_{a'\\in A} Q^{old}(s_t, a') - Q^{old}(s_t, a_t) \\right) .\n",
    "$$\n",
    "\n",
    "With function approximation, meaning model $Q(s,a)$ with a function $Q_{\\theta}(s,a)$ parameterized by $\\theta$, we arrive at the Fitted Q Iteration (FQI) algorithm, or better known as Deep Q Learning if the function class is neural networks. Q-Learning with neural network as function approximator was known long ago, but it was only recently (year 2013) that DeepMind made this algorithm actually work on Atari games. Deep Q Learning iteratively optimize the following objective:\n",
    "$$\n",
    "\\theta_{new} \\leftarrow \\arg\\min_{\\theta} \\mathbb{E}_{(s,a,r,s')\\sim D} \\left( r + \\gamma \\max_{a'\\in A} Q_{\\theta_{old}}(s', a') - Q_{\\theta}(s, a) \\right)^2  .\n",
    "$$\n",
    "\n",
    "Therefore, with a batch of $\\{(s^i,a^i,r^i,s'^i)\\}_{i=1}^N$ sampled from the replay buffer, we can build a loss function $L$ in pytorch:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N \\left( r^i + \\gamma \\max_{a'\\in A} Q_{\\theta_{old}}(s'^i, a') - Q_{\\theta}(s^i, a^i) \\right)^2\n",
    ",\n",
    "$$\n",
    "and run the usual gradient descent on $\\theta$ with a pytorch optimizer.\n",
    "\n",
    "\n",
    "#### Exploration\n",
    "Exploration, as the word suggests, refers to explore novel unvisited states in RL. The FQI (or DQN) needs an exploratory datasets to work well. The common way to produce exploratory dataset is through randomization, such as the $\\epsilon$-greedy exploration strategy we will implement in this assignment.\n",
    "- $\\epsilon$-greedy exploration:\n",
    "\n",
    "At training iteration $it$, the agent chooses to play\n",
    "$$\n",
    "a = \\begin{cases}\n",
    "\\arg\\max_a Q_{\\theta}(s, a)      &  \\text{ with probability $1 - \\epsilon_{it}$ },  \\\\\n",
    "\\text{a random action $a \\in A$} &  \\text{ with probability $\\epsilon_{it}$ }.  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "And $\\epsilon_{it}$ is annealed, for example, linearly from $1$ to $0.01$ as training progresses until iteration $it_{\\text{decay}}$:\n",
    "$$\n",
    "\\epsilon_{it} = \\max\\Big\\{ 0.01, 1 + (0.01-1)\\frac{it}{it_{\\text{decay}}} \\Big\\}.\n",
    "$$\n",
    "\n",
    "#### Two Caveats\n",
    "1. There's an improvement on DQN called Double-DQN with the following loss $L$, which has shown to be empirically more stable than the original DQN loss described above. You may want to implement the improved one in your code:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N \\left( r^i + \\gamma Q_{\\theta_{old}}\\big( s'^i, \\arg\\max_{a'\\in A} Q_{\\theta}(s'^i, a' ) \\big) - Q_{\\theta}(s^i, a^i) \\right)^2\n",
    ".\n",
    "$$\n",
    "2. Huber loss (a.k.a smooth L1 loss) is commonly used to reduce the effect of extreme values:\n",
    "$$\n",
    "L(\\theta) = \\frac1N \\sum_{i=1}^N Huber\\left( r^i + \\gamma Q_{\\theta_{old}}\\big( s'^i, \\arg\\max_{a'\\in A} Q_{\\theta}(s'^i, a' ) \\big) - Q_{\\theta}(s^i, a^i) \\right)\n",
    "$$\n",
    "You can look up the pytorch document here: https://pytorch.org/docs/stable/nn.functional.html#smooth-l1-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C1 (5 pts): Complete the code for the two layered fully connected network class `TwoLayerFCNet` in file `Model.py`\n",
    "And run the cell below to test the output shape of your module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape test passed!\n"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Model import TwoLayerFCNet\n",
    "import torch\n",
    "net = TwoLayerFCNet(n_in=4, n_hidden=16, n_out=5)\n",
    "x = torch.randn(10, 4)\n",
    "y = net(x)\n",
    "assert y.shape == (10, 5), \"ERROR: network output has the wrong shape!\"\n",
    "print (\"Output shape test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C2 (5 pts): Complete the code for $\\epsilon$-greedy exploration strategy in function `DQN.act` in file `Algo.py'\n",
    "And run the cell below to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-greedy test passed!\n"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Algo import DQN\n",
    "class Nothing: pass\n",
    "dummy = Nothing()\n",
    "dummy.eps_decay = 500000\n",
    "\n",
    "dummy.num_act_steps = 0\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 1.0 ) < 0.01, \"ERROR: compute_epsilon at t=0 should be 1 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 250000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.505 ) < 0.01, \"ERROR: compute_epsilon at t=250000 should around .505 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 500000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.01 ) < 0.01, \"ERROR: compute_epsilon at t=500000 should be .01 but got %f!\" % eps\n",
    "\n",
    "dummy.num_act_steps = 600000\n",
    "eps = DQN.compute_epsilon(dummy)\n",
    "assert abs( eps - 0.01 ) < 0.01, \"ERROR: compute_epsilon after t=500000 should be .01 but got %f!\" % eps\n",
    "print (\"Epsilon-greedy test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C3 (10 pts): Complete the code for computing the loss function in `DQN.train` in file `Algo.py`\n",
    "And run the cell below to verify your code decreses the loss value in one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters to optimize: [('fc1.weight', torch.Size([128, 10]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([3, 128]), True), ('fc2.bias', torch.Size([3]), True)] \n",
      "\n",
      "0.4453107714653015 > 0.44043222069740295 ?\n",
      "DQN.train test passed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Algo import DQN\n",
    "class Nothing: pass\n",
    "dummy_obs_space, dummy_act_space = Nothing(), Nothing()\n",
    "dummy_obs_space.shape = [10]\n",
    "dummy_act_space.n = 3\n",
    "\n",
    "dqn = DQN(dummy_obs_space, dummy_act_space, batch_size=2)\n",
    "\n",
    "for t in range(3):\n",
    "    dqn.observe([np.random.randn(10).astype('float32')], [np.random.randint(3)],\n",
    "                [(np.random.randn(10).astype('float32'), np.random.rand(), False, None)])\n",
    "\n",
    "b = dqn.replay.cur_batch\n",
    "loss1 = dqn.train()\n",
    "dqn.replay.cur_batch = b\n",
    "loss2 = dqn.train()\n",
    "\n",
    "print (loss1, '>', loss2, '?')\n",
    "assert loss2 < loss1, \"DQN.train should reduce loss on the same batch\"\n",
    "\n",
    "print (\"DQN.train test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P1 (10 pts): Run DQN on CartPole and plot the learning curve (i.e. averaged episodic reward against env steps).\n",
    "Your code should be able to achieve **>150** averaged reward in 10000 iterations (20000 simulation steps) in only a few minutes. This is a good indication that the implementation is correct. It's ok that the curve is not always monotonically increasing because of randomness in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(algo='dqn', batch_size=64, checkpoint_freq=20000, discount=0.996, ent_coef=0.01, env='CartPole-v1', eps_decay=4000, frame_skip=1, frame_stack=4, load='', log='log.txt', lr=0.001, niter=10000, nproc=2, parallel_env=0, print_freq=200, replay_size=20000, save_dir='cartpole_dqn/', target_update=1000, train_freq=1, train_start=100, value_coef=0.5)\n",
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "running on device cpu\n",
      "parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True)] \n",
      "\n",
      "obses on reset: 2 x (4,) float32\n",
      "iter    200 |loss   0.02 |n_ep    14 |ep_len   25.8 |ep_rew  25.77 |raw_ep_rew  25.77 |env_step    400 |time 00:00 rem 00:11\n",
      "iter    400 |loss   0.00 |n_ep    34 |ep_len   24.8 |ep_rew  24.78 |raw_ep_rew  24.78 |env_step    800 |time 00:00 rem 00:16\n",
      "iter    600 |loss   0.00 |n_ep    51 |ep_len   23.7 |ep_rew  23.74 |raw_ep_rew  23.74 |env_step   1200 |time 00:01 rem 00:16\n",
      "iter    800 |loss   0.00 |n_ep    70 |ep_len   19.2 |ep_rew  19.21 |raw_ep_rew  19.21 |env_step   1600 |time 00:01 rem 00:16\n",
      "iter   1000 |loss   0.00 |n_ep    93 |ep_len   16.6 |ep_rew  16.61 |raw_ep_rew  16.61 |env_step   2000 |time 00:01 rem 00:16\n",
      "iter   1200 |loss   0.03 |n_ep   112 |ep_len   18.8 |ep_rew  18.82 |raw_ep_rew  18.82 |env_step   2400 |time 00:02 rem 00:16\n",
      "iter   1400 |loss   0.04 |n_ep   136 |ep_len   17.8 |ep_rew  17.85 |raw_ep_rew  17.85 |env_step   2800 |time 00:02 rem 00:16\n",
      "iter   1600 |loss   0.03 |n_ep   160 |ep_len   16.2 |ep_rew  16.24 |raw_ep_rew  16.24 |env_step   3200 |time 00:03 rem 00:16\n",
      "iter   1800 |loss   0.03 |n_ep   185 |ep_len   15.4 |ep_rew  15.38 |raw_ep_rew  15.38 |env_step   3600 |time 00:03 rem 00:15\n",
      "iter   2000 |loss   0.02 |n_ep   211 |ep_len   15.2 |ep_rew  15.17 |raw_ep_rew  15.17 |env_step   4000 |time 00:03 rem 00:15\n",
      "iter   2200 |loss   0.06 |n_ep   242 |ep_len   12.9 |ep_rew  12.93 |raw_ep_rew  12.93 |env_step   4400 |time 00:04 rem 00:15\n",
      "iter   2400 |loss   0.13 |n_ep   264 |ep_len   16.3 |ep_rew  16.26 |raw_ep_rew  16.26 |env_step   4800 |time 00:04 rem 00:15\n",
      "iter   2600 |loss   0.12 |n_ep   282 |ep_len   19.5 |ep_rew  19.48 |raw_ep_rew  19.48 |env_step   5200 |time 00:05 rem 00:14\n",
      "iter   2800 |loss   0.24 |n_ep   304 |ep_len   17.5 |ep_rew  17.54 |raw_ep_rew  17.54 |env_step   5600 |time 00:05 rem 00:13\n",
      "iter   3000 |loss   0.03 |n_ep   323 |ep_len   17.2 |ep_rew  17.23 |raw_ep_rew  17.23 |env_step   6000 |time 00:05 rem 00:13\n",
      "iter   3200 |loss   0.12 |n_ep   333 |ep_len   32.6 |ep_rew  32.60 |raw_ep_rew  32.60 |env_step   6400 |time 00:06 rem 00:13\n",
      "iter   3400 |loss   0.08 |n_ep   340 |ep_len   41.5 |ep_rew  41.53 |raw_ep_rew  41.53 |env_step   6800 |time 00:06 rem 00:12\n",
      "iter   3600 |loss   0.08 |n_ep   346 |ep_len   59.1 |ep_rew  59.07 |raw_ep_rew  59.07 |env_step   7200 |time 00:07 rem 00:12\n",
      "iter   3800 |loss   0.03 |n_ep   352 |ep_len   61.3 |ep_rew  61.34 |raw_ep_rew  61.34 |env_step   7600 |time 00:07 rem 00:12\n",
      "iter   4000 |loss   0.05 |n_ep   358 |ep_len   62.6 |ep_rew  62.60 |raw_ep_rew  62.60 |env_step   8000 |time 00:08 rem 00:12\n",
      "iter   4200 |loss   0.21 |n_ep   363 |ep_len   69.0 |ep_rew  68.95 |raw_ep_rew  68.95 |env_step   8400 |time 00:09 rem 00:12\n",
      "iter   4400 |loss   0.36 |n_ep   367 |ep_len   75.2 |ep_rew  75.22 |raw_ep_rew  75.22 |env_step   8800 |time 00:09 rem 00:12\n",
      "iter   4600 |loss   0.21 |n_ep   371 |ep_len   87.0 |ep_rew  86.96 |raw_ep_rew  86.96 |env_step   9200 |time 00:10 rem 00:12\n",
      "iter   4800 |loss   0.16 |n_ep   375 |ep_len   91.8 |ep_rew  91.75 |raw_ep_rew  91.75 |env_step   9600 |time 00:11 rem 00:12\n",
      "iter   5000 |loss   0.05 |n_ep   380 |ep_len   91.6 |ep_rew  91.64 |raw_ep_rew  91.64 |env_step  10000 |time 00:11 rem 00:11\n",
      "iter   5200 |loss   0.19 |n_ep   384 |ep_len   95.5 |ep_rew  95.47 |raw_ep_rew  95.47 |env_step  10400 |time 00:12 rem 00:11\n",
      "iter   5400 |loss   0.24 |n_ep   387 |ep_len   99.0 |ep_rew  98.95 |raw_ep_rew  98.95 |env_step  10800 |time 00:13 rem 00:11\n",
      "iter   5600 |loss   0.02 |n_ep   388 |ep_len  110.5 |ep_rew 110.46 |raw_ep_rew 110.46 |env_step  11200 |time 00:14 rem 00:11\n",
      "iter   5800 |loss   0.21 |n_ep   390 |ep_len  136.5 |ep_rew 136.48 |raw_ep_rew 136.48 |env_step  11600 |time 00:15 rem 00:10\n",
      "iter   6000 |loss   0.02 |n_ep   393 |ep_len  137.6 |ep_rew 137.57 |raw_ep_rew 137.57 |env_step  12000 |time 00:16 rem 00:10\n",
      "iter   6200 |loss   0.20 |n_ep   396 |ep_len  149.0 |ep_rew 149.05 |raw_ep_rew 149.05 |env_step  12400 |time 00:16 rem 00:10\n",
      "iter   6400 |loss   0.04 |n_ep   398 |ep_len  152.7 |ep_rew 152.66 |raw_ep_rew 152.66 |env_step  12800 |time 00:17 rem 00:10\n",
      "iter   6600 |loss   0.04 |n_ep   400 |ep_len  155.1 |ep_rew 155.10 |raw_ep_rew 155.10 |env_step  13200 |time 00:18 rem 00:09\n",
      "iter   6800 |loss   0.15 |n_ep   402 |ep_len  156.1 |ep_rew 156.11 |raw_ep_rew 156.11 |env_step  13600 |time 00:19 rem 00:09\n",
      "iter   7000 |loss   0.35 |n_ep   404 |ep_len  157.0 |ep_rew 156.97 |raw_ep_rew 156.97 |env_step  14000 |time 00:20 rem 00:08\n",
      "iter   7200 |loss   0.03 |n_ep   407 |ep_len  163.5 |ep_rew 163.50 |raw_ep_rew 163.50 |env_step  14400 |time 00:21 rem 00:08\n",
      "iter   7400 |loss   0.18 |n_ep   410 |ep_len  167.7 |ep_rew 167.74 |raw_ep_rew 167.74 |env_step  14800 |time 00:21 rem 00:07\n",
      "iter   7600 |loss   0.32 |n_ep   411 |ep_len  170.4 |ep_rew 170.36 |raw_ep_rew 170.36 |env_step  15200 |time 00:22 rem 00:07\n",
      "iter   7800 |loss   0.37 |n_ep   413 |ep_len  183.6 |ep_rew 183.61 |raw_ep_rew 183.61 |env_step  15600 |time 00:23 rem 00:06\n",
      "iter   8000 |loss   0.02 |n_ep   415 |ep_len  191.4 |ep_rew 191.41 |raw_ep_rew 191.41 |env_step  16000 |time 00:24 rem 00:06\n",
      "iter   8200 |loss   0.48 |n_ep   417 |ep_len  188.7 |ep_rew 188.67 |raw_ep_rew 188.67 |env_step  16400 |time 00:25 rem 00:05\n",
      "iter   8400 |loss   0.06 |n_ep   418 |ep_len  196.8 |ep_rew 196.80 |raw_ep_rew 196.80 |env_step  16800 |time 00:26 rem 00:05\n",
      "iter   8600 |loss   0.09 |n_ep   419 |ep_len  205.7 |ep_rew 205.72 |raw_ep_rew 205.72 |env_step  17200 |time 00:27 rem 00:04\n",
      "iter   8800 |loss   0.13 |n_ep   421 |ep_len  210.6 |ep_rew 210.62 |raw_ep_rew 210.62 |env_step  17600 |time 00:28 rem 00:03\n",
      "iter   9000 |loss   0.07 |n_ep   423 |ep_len  212.6 |ep_rew 212.64 |raw_ep_rew 212.64 |env_step  18000 |time 00:29 rem 00:03\n",
      "iter   9200 |loss   0.03 |n_ep   424 |ep_len  216.4 |ep_rew 216.37 |raw_ep_rew 216.37 |env_step  18400 |time 00:29 rem 00:02\n",
      "iter   9400 |loss   0.03 |n_ep   427 |ep_len  225.5 |ep_rew 225.46 |raw_ep_rew 225.46 |env_step  18800 |time 00:30 rem 00:01\n",
      "iter   9600 |loss   0.24 |n_ep   428 |ep_len  225.8 |ep_rew 225.82 |raw_ep_rew 225.82 |env_step  19200 |time 00:31 rem 00:01\n",
      "iter   9800 |loss   0.08 |n_ep   430 |ep_len  232.5 |ep_rew 232.51 |raw_ep_rew 232.51 |env_step  19600 |time 00:32 rem 00:00\n",
      "save checkpoint to cartpole_dqn/9999.pth\n"
     ]
    }
   ],
   "source": [
    "%run Main.py  \\\n",
    "    --niter 10000   \\\n",
    "    --env CartPole-v1   \\\n",
    "    --algo dqn  \\\n",
    "    --nproc 2   \\\n",
    "    --lr 0.001  \\\n",
    "    --train_freq 1  \\\n",
    "    --train_start 100   \\\n",
    "    --replay_size 20000 \\\n",
    "    --batch_size 64     \\\n",
    "    --discount 0.996    \\\n",
    "    --target_update 1000    \\\n",
    "    --eps_decay 4000    \\\n",
    "    --print_freq 200    \\\n",
    "    --checkpoint_freq 20000 \\\n",
    "    --save_dir cartpole_dqn \\\n",
    "    --log log.txt \\\n",
    "    --parallel_env 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curve(logfile, title=None):\n",
    "    lines = open(logfile, 'r').readlines()\n",
    "    lines = [l.split() for l in lines if l[:4] == 'iter']\n",
    "    steps = [int(l[13]) for l in lines]\n",
    "    rewards = [float(l[11]) for l in lines]\n",
    "    plt.plot(steps, rewards)\n",
    "    plt.xlabel('env steps'); plt.ylabel('avg episode reward'); plt.grid(True)\n",
    "    if title: plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log is saved to `'cartpole_dqn/log.txt'`. Let's plot the running averaged episode reward curve during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve('cartpole_dqn/log.txt', 'CartPole DQN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Actor-Critic Algorithm\n",
    "\n",
    "Policy gradient methods are another class of algorithms that originated from viewing the RL problem as a mathematical optimization problem. Recall that the objective of RL is to maximize the expected cumulative reward the agent gets, namely\n",
    "$$\n",
    "\\max_{\\pi} J(\\pi) := \\mathbb{E}_{ (s_t,a_t,r_t)\\sim D^{\\pi} } \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right]\n",
    "$$\n",
    "where $D^{\\pi}$ is the distribution of trajectories induced by policy $\\pi$, and inside the expectation is the random variable representing the discounted cumulative reward and $J$ is the reward (or cost) functional. Essentially, we want to optimize the policy $\\pi$.\n",
    "\n",
    "The most straightforward way is to run gradient update on the parameter $\\theta$ of a *parameterized* policy $\\pi_{\\theta}$. One such algorithm is the so-called `Advantage Actor-Critic (A2C)`. A2C is an on-policy policy optimization type algorithm. While collecting on-policy data, we iteratively run gradient ascent:\n",
    "$$\n",
    "\\theta_{new} \\leftarrow \\theta_{old} + \\eta { \\hat \\nabla_{\\theta} } J(\\pi_{\\theta_{old}})\n",
    "$$\n",
    "with a Monte Carlo estimate ${ \\hat \\nabla_{\\theta} } J$ of the true gradient $\\nabla_{\\theta} J$. The true gradient writes as (by Policy Gradient Theorem and some manipulations):\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\pi_{\\theta_{old}}) = \\mathbb{E}_{ (s_t,a_t,r_t)\\sim D^{ \\pi_{\\theta_{old}} } } \\sum_{t=0}^{\\infty} \\left( \\nabla_{\\theta} \\log \\pi_{\\theta_{old}} (s_t, a_t) \\left( \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'} - V^{ \\pi_{\\theta_{old}} }(s_t) \\right) \\right)  .\n",
    "$$\n",
    "The quantity in the inner-most parentheses $A(s_t, a_t) = Q(s_t, a_t) - V(s_t) = (\\mathbb{E} \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'}) - V(s_t)$ is called the *Advantage* function (not very rigoriously speaking...). That's why it's called **Advantage** Actor-Critic. More on A2C: https://arxiv.org/abs/1506.02438.\n",
    "\n",
    "And the Monte Carlo estimate of the gradient is\n",
    "$$\n",
    "{ \\hat \\nabla_{\\theta} } J(\\pi_{\\theta_{old}}) = \\frac1{NT}  \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\nabla_{\\theta} \\log \\pi_{\\theta_{old}} (s_t^{i}, a_t^{i}) \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi_{old}}(s_t^{i}) \\right) \\right)\n",
    "$$\n",
    "where $V_{\\phi_{old}}$ is introduced as a *parameterized* estimate for $V^{ \\pi_{\\theta_{old}} }$, which can also be a neural network. So $V_{\\phi}$ is the **critic** and $\\pi_{\\theta}$ is the **actor**. We can construct a specific loss function in pytorch that gives ${ \\hat \\nabla_{\\theta} } J$. $V_{\\phi_{old}}$ is trained with SGD on a L2 loss function. It's further common practice to add an entropy bonus loss term to encourage maximum entropy solution, to facilitate exploration and avoid getting stuck in local minima. We shall clarify these loss functions in the following summarization.\n",
    "\n",
    "#### Summarizing a variant of the A2C algorithm:\n",
    "> For many iterations repeat:\n",
    "1. Collect $N$ independent trajectories $\\{ (s_t^{i},a_t^{i},r_t^{i})_{t=0}^T \\}_{i=1}^{N}$ by running policy $\\pi_{\\theta}$ for maximum $T$ steps;\n",
    "2. Compute the loss function for the policy parameter $\\theta$:\n",
    "$$\n",
    "L_{policy}(\\theta) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\log \\pi_{\\theta} (s_t^{i}, a_t^{i}) \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi}(s_t^{i}) \\right) \\right)\n",
    "$$\n",
    "Compute the entropy term for $\\theta$:\n",
    "$$\n",
    "L_{entropy}(\\theta) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( - \\sum_{a\\in A} \\pi_{\\theta}(s_t^{i}, a) \\log \\pi_{\\theta}(s_t^{i}, a) \\right)\n",
    "$$\n",
    "Compute the loss for value function parameter $\\phi$:\n",
    "$$\n",
    "L_{value}(\\phi) = \\frac1{NT} \\sum_{i=1}^N \\sum_{t=0}^T \\left( \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}^{i} - V_{\\phi}(s_t^{i}) \\right)^2\n",
    "$$\n",
    "3. Use pytorch auto-differentiation and optimizer to do one gradient step on $(\\theta, \\phi)$ with the overall loss:\n",
    "$$\n",
    "L(\\theta, \\phi) = - L_{policy} - \\lambda_{ent} L_{entropy} + \\lambda_{val} L_{value}\n",
    "$$\n",
    "where $\\lambda_{ent}$ and $\\lambda_{val}$ are coefficients to balances the loss terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C4 (10 pts): Complete the code for computing the advantange, entropy and loss function in `A2C.train` in file `Algo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P2 (10 pts): Run A2C on CartPole and plot the learning curve (i.e. averaged episodic reward against training iteration).\n",
    "Your code should be able to achieve **>150** averaged reward in 10000 iterations (40000 simulation steps) in only a few minutes. This is a good indication that the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(algo='a2c', batch_size=64, checkpoint_freq=20000, discount=0.996, ent_coef=0.01, env='CartPole-v1', eps_decay=200000, frame_skip=1, frame_stack=4, load='', log='log.txt', lr=0.001, niter=10000, nproc=4, parallel_env=0, print_freq=200, replay_size=1000000, save_dir='cartpole_a2c/', target_update=2500, train_freq=16, train_start=0, value_coef=0.01)\n",
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n",
      "running on device cpu\n",
      "shared net = False, parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True), ('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([1, 128]), True), ('fc2.bias', torch.Size([1]), True)] \n",
      "\n",
      "obses on reset: 4 x (4,) float32\n",
      "iter    200 |loss   0.76 |n_ep    47 |ep_len   17.7 |ep_rew  17.65 |raw_ep_rew  17.65 |env_step    800 |time 00:00 rem 00:14\n",
      "iter    400 |loss   1.00 |n_ep    84 |ep_len   20.5 |ep_rew  20.53 |raw_ep_rew  20.53 |env_step   1600 |time 00:00 rem 00:13\n",
      "iter    600 |loss   0.72 |n_ep   119 |ep_len   22.6 |ep_rew  22.64 |raw_ep_rew  22.64 |env_step   2400 |time 00:00 rem 00:13\n",
      "iter    800 |loss   0.68 |n_ep   153 |ep_len   27.4 |ep_rew  27.39 |raw_ep_rew  27.39 |env_step   3200 |time 00:01 rem 00:13\n",
      "iter   1000 |loss   0.63 |n_ep   190 |ep_len   18.7 |ep_rew  18.75 |raw_ep_rew  18.75 |env_step   4000 |time 00:01 rem 00:13\n",
      "iter   1200 |loss   0.82 |n_ep   223 |ep_len   23.1 |ep_rew  23.15 |raw_ep_rew  23.15 |env_step   4800 |time 00:01 rem 00:13\n",
      "iter   1400 |loss   0.60 |n_ep   258 |ep_len   24.9 |ep_rew  24.92 |raw_ep_rew  24.92 |env_step   5600 |time 00:02 rem 00:13\n",
      "iter   1600 |loss   0.83 |n_ep   285 |ep_len   26.1 |ep_rew  26.14 |raw_ep_rew  26.14 |env_step   6400 |time 00:02 rem 00:12\n",
      "iter   1800 |loss   0.61 |n_ep   312 |ep_len   30.6 |ep_rew  30.61 |raw_ep_rew  30.61 |env_step   7200 |time 00:02 rem 00:12\n",
      "iter   2000 |loss   0.55 |n_ep   338 |ep_len   32.3 |ep_rew  32.29 |raw_ep_rew  32.29 |env_step   8000 |time 00:03 rem 00:12\n",
      "iter   2200 |loss   0.53 |n_ep   360 |ep_len   34.5 |ep_rew  34.51 |raw_ep_rew  34.51 |env_step   8800 |time 00:03 rem 00:11\n",
      "iter   2400 |loss   0.71 |n_ep   387 |ep_len   31.5 |ep_rew  31.46 |raw_ep_rew  31.46 |env_step   9600 |time 00:03 rem 00:11\n",
      "iter   2600 |loss   0.80 |n_ep   409 |ep_len   35.4 |ep_rew  35.44 |raw_ep_rew  35.44 |env_step  10400 |time 00:03 rem 00:11\n",
      "iter   2800 |loss   0.38 |n_ep   428 |ep_len   46.0 |ep_rew  46.05 |raw_ep_rew  46.05 |env_step  11200 |time 00:04 rem 00:10\n",
      "iter   3000 |loss   0.52 |n_ep   439 |ep_len   64.3 |ep_rew  64.27 |raw_ep_rew  64.27 |env_step  12000 |time 00:04 rem 00:10\n",
      "iter   3200 |loss   0.82 |n_ep   447 |ep_len   71.2 |ep_rew  71.25 |raw_ep_rew  71.25 |env_step  12800 |time 00:04 rem 00:10\n",
      "iter   3400 |loss   0.34 |n_ep   460 |ep_len   72.1 |ep_rew  72.14 |raw_ep_rew  72.14 |env_step  13600 |time 00:05 rem 00:09\n",
      "iter   3600 |loss   0.27 |n_ep   470 |ep_len   82.6 |ep_rew  82.59 |raw_ep_rew  82.59 |env_step  14400 |time 00:05 rem 00:09\n",
      "iter   3800 |loss   0.38 |n_ep   481 |ep_len   74.9 |ep_rew  74.89 |raw_ep_rew  74.89 |env_step  15200 |time 00:05 rem 00:09\n",
      "iter   4000 |loss   0.23 |n_ep   492 |ep_len   71.0 |ep_rew  71.05 |raw_ep_rew  71.05 |env_step  16000 |time 00:06 rem 00:09\n",
      "iter   4200 |loss   0.99 |n_ep   498 |ep_len   79.3 |ep_rew  79.31 |raw_ep_rew  79.31 |env_step  16800 |time 00:06 rem 00:08\n",
      "iter   4400 |loss   0.96 |n_ep   507 |ep_len   79.6 |ep_rew  79.61 |raw_ep_rew  79.61 |env_step  17600 |time 00:06 rem 00:08\n",
      "iter   4600 |loss   0.95 |n_ep   516 |ep_len   95.7 |ep_rew  95.73 |raw_ep_rew  95.73 |env_step  18400 |time 00:07 rem 00:08\n",
      "iter   4800 |loss   0.69 |n_ep   524 |ep_len   88.0 |ep_rew  87.97 |raw_ep_rew  87.97 |env_step  19200 |time 00:07 rem 00:08\n",
      "iter   5000 |loss   0.59 |n_ep   530 |ep_len  108.3 |ep_rew 108.25 |raw_ep_rew 108.25 |env_step  20000 |time 00:08 rem 00:08\n",
      "iter   5200 |loss   0.76 |n_ep   536 |ep_len  122.0 |ep_rew 121.98 |raw_ep_rew 121.98 |env_step  20800 |time 00:08 rem 00:07\n",
      "iter   5400 |loss   0.86 |n_ep   541 |ep_len  131.6 |ep_rew 131.55 |raw_ep_rew 131.55 |env_step  21600 |time 00:09 rem 00:07\n",
      "iter   5600 |loss   0.88 |n_ep   545 |ep_len  159.2 |ep_rew 159.18 |raw_ep_rew 159.18 |env_step  22400 |time 00:09 rem 00:07\n",
      "iter   5800 |loss   0.28 |n_ep   551 |ep_len  156.5 |ep_rew 156.45 |raw_ep_rew 156.45 |env_step  23200 |time 00:09 rem 00:07\n",
      "iter   6000 |loss  -0.04 |n_ep   554 |ep_len  174.5 |ep_rew 174.48 |raw_ep_rew 174.48 |env_step  24000 |time 00:10 rem 00:06\n",
      "iter   6200 |loss   0.75 |n_ep   558 |ep_len  181.2 |ep_rew 181.16 |raw_ep_rew 181.16 |env_step  24800 |time 00:10 rem 00:06\n",
      "iter   6400 |loss   0.02 |n_ep   562 |ep_len  181.8 |ep_rew 181.80 |raw_ep_rew 181.80 |env_step  25600 |time 00:10 rem 00:06\n",
      "iter   6600 |loss   0.87 |n_ep   566 |ep_len  184.0 |ep_rew 184.00 |raw_ep_rew 184.00 |env_step  26400 |time 00:11 rem 00:05\n",
      "iter   6800 |loss   0.89 |n_ep   569 |ep_len  188.9 |ep_rew 188.93 |raw_ep_rew 188.93 |env_step  27200 |time 00:11 rem 00:05\n",
      "iter   7000 |loss  -0.10 |n_ep   575 |ep_len  187.3 |ep_rew 187.34 |raw_ep_rew 187.34 |env_step  28000 |time 00:11 rem 00:05\n",
      "iter   7200 |loss   0.77 |n_ep   579 |ep_len  200.7 |ep_rew 200.71 |raw_ep_rew 200.71 |env_step  28800 |time 00:12 rem 00:04\n",
      "iter   7400 |loss   0.74 |n_ep   583 |ep_len  182.9 |ep_rew 182.89 |raw_ep_rew 182.89 |env_step  29600 |time 00:12 rem 00:04\n",
      "iter   7600 |loss   0.60 |n_ep   588 |ep_len  186.0 |ep_rew 186.03 |raw_ep_rew 186.03 |env_step  30400 |time 00:13 rem 00:04\n",
      "iter   7800 |loss   0.30 |n_ep   594 |ep_len  154.5 |ep_rew 154.54 |raw_ep_rew 154.54 |env_step  31200 |time 00:13 rem 00:03\n",
      "iter   8000 |loss   0.62 |n_ep   599 |ep_len  150.0 |ep_rew 150.04 |raw_ep_rew 150.04 |env_step  32000 |time 00:13 rem 00:03\n",
      "iter   8200 |loss   0.87 |n_ep   601 |ep_len  163.8 |ep_rew 163.84 |raw_ep_rew 163.84 |env_step  32800 |time 00:14 rem 00:03\n",
      "iter   8400 |loss   0.09 |n_ep   606 |ep_len  197.9 |ep_rew 197.86 |raw_ep_rew 197.86 |env_step  33600 |time 00:14 rem 00:02\n",
      "iter   8600 |loss   0.80 |n_ep   607 |ep_len  186.2 |ep_rew 186.17 |raw_ep_rew 186.17 |env_step  34400 |time 00:14 rem 00:02\n",
      "iter   8800 |loss   0.94 |n_ep   611 |ep_len  208.6 |ep_rew 208.60 |raw_ep_rew 208.60 |env_step  35200 |time 00:15 rem 00:02\n",
      "iter   9000 |loss   0.10 |n_ep   615 |ep_len  245.6 |ep_rew 245.59 |raw_ep_rew 245.59 |env_step  36000 |time 00:15 rem 00:01\n",
      "iter   9200 |loss   0.87 |n_ep   616 |ep_len  248.2 |ep_rew 248.23 |raw_ep_rew 248.23 |env_step  36800 |time 00:16 rem 00:01\n",
      "iter   9400 |loss   0.75 |n_ep   618 |ep_len  280.4 |ep_rew 280.41 |raw_ep_rew 280.41 |env_step  37600 |time 00:16 rem 00:01\n",
      "iter   9600 |loss   0.93 |n_ep   622 |ep_len  277.8 |ep_rew 277.84 |raw_ep_rew 277.84 |env_step  38400 |time 00:16 rem 00:00\n",
      "iter   9800 |loss   0.39 |n_ep   623 |ep_len  300.1 |ep_rew 300.05 |raw_ep_rew 300.05 |env_step  39200 |time 00:17 rem 00:00\n",
      "save checkpoint to cartpole_a2c/9999.pth\n"
     ]
    }
   ],
   "source": [
    "%run Main.py  \\\n",
    "    --niter 10000   \\\n",
    "    --env CartPole-v1   \\\n",
    "    --algo a2c  \\\n",
    "    --nproc 4   \\\n",
    "    --lr 0.001  \\\n",
    "    --train_freq 16 \\\n",
    "    --train_start 0 \\\n",
    "    --batch_size 64     \\\n",
    "    --discount 0.996    \\\n",
    "    --value_coef 0.01    \\\n",
    "    --print_freq 200    \\\n",
    "    --checkpoint_freq 20000 \\\n",
    "    --save_dir cartpole_a2c \\\n",
    "    --log log.txt \\\n",
    "    --parallel_env 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curve('cartpole_a2c/log.txt', 'CartPole A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play a little bit with the trained agent. The neural net parameters are saved to the `cartpole_dqn` and `cartpole_a2c` folders. The cell below will open a window showing one episode play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared net = False, parameters to optimize: [('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([2, 128]), True), ('fc2.bias', torch.Size([2]), True), ('fc1.weight', torch.Size([128, 4]), True), ('fc1.bias', torch.Size([128]), True), ('fc2.weight', torch.Size([1, 128]), True), ('fc2.bias', torch.Size([1]), True)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import Algo\n",
    "env = gym.make('CartPole-v1')\n",
    "agent = Algo.ActorCritic(env.observation_space, env.action_space)\n",
    "agent.load('cartpole_a2c/9999.pth')\n",
    "state = env.reset()\n",
    "for _ in range(120):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(agent.act([state])[0])\n",
    "    if done: break\n",
    "    time.sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Solve the Atari Breakout game\n",
    "***\n",
    "In this part, you'll train your agent to play Breakout with the BlueWaters cluster. I have provided the job scripts for you. Please upload your `Algo.py` and `Model.py` completed in **Part I** to your BlueWaters folder. And submit the following two jobs respectively:\n",
    "```\n",
    "qsub run_dqn.pbs\n",
    "qsub run_a2c.pbs\n",
    "```\n",
    "\n",
    "The jobs are set to run for at most **14 hours**. **<font color=red>Please start early!!</font>** You might be able to reach the desired score (>= 200 reward) before 14 hours - You can stop the training early if you wish. Then please collect the resulting `breakout_dqn/log.txt` and `breakout_a2c/log.txt` files into the same folder as this Jupyter notebook's. Rename them as `log_breakout_dqn.txt` and `log_breakout_a2c.txt`.\n",
    "\n",
    "BTW, there's an Atari PC simulator: https://stella-emu.github.io/ I spent a lot of time playing them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C5 (10 pts): Complete the code for the CNN with 3 conv layers and 3 fc layers in class `SimpleCNN` in file `Model.py`\n",
    "And verify the output shape with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN output shape test passed!\n"
     ]
    }
   ],
   "source": [
    "## Test code\n",
    "from Model import SimpleCNN\n",
    "import torch\n",
    "net = SimpleCNN()\n",
    "x = torch.randn(2, 4, 84, 84)\n",
    "y = net(x)\n",
    "assert y.shape == (2, 4), \"ERROR: network output has the wrong shape!\"\n",
    "print (\"CNN output shape test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P3 (10 pts): Run the following cell to generate a DQN learning curve.\n",
    "The *maximum* average episodic reward on this curve should be larger than $200$ for full credit. (It's ok if the final reward is not as high.) The typical value is around $300$. You get 70% credit if $100 \\le$ average episodic reward $< 200$, 50% credit if $50 \\le$ average episodic reward $< 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'log_breakout_dqn.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d8074d0ac4c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_breakout_dqn.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Breakout DQN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-272a5cc34516>\u001b[0m in \u001b[0;36mplot_curve\u001b[1;34m(logfile, title)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'iter'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'log_breakout_dqn.txt'"
     ]
    }
   ],
   "source": [
    "plot_curve('log_breakout_dqn.txt', 'Breakout DQN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P4 (10 pts): Run the following cell to generate an A2C learning curve.\n",
    "The *maximum* average episodic reward on this curve should be larger than $150$ for full credit. (It's ok if the final reward is not as high.) The typical value is around $250$. You get 70% credit if $50 \\le$ average episodic reward $< 150$, and 50% credit if $20 \\le$ average episodic reward $< 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve('log_breakout_a2c.txt', 'Breakout A2C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P5 (10 pts): Collect and visualize some game frames by running the script `Draw.py` on BlueWaters.\n",
    "(1) `module load python/2.0.0` and run `Draw.py` on BlueWaters (it's ok to run this locally, no need to start a job).\n",
    "\n",
    "(2) Download the result `breakout_imgs` folder from BlueWaters to the folder containing this Jupyter notebook, and run the following cell. You should see some animation of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "imgs = sorted(os.listdir('breakout_imgs'))\n",
    "#imgs = [plt.imread('breakout_imgs/' + img) for img in imgs]\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "pimg = None\n",
    "for img in imgs:\n",
    "    img = plt.imread('breakout_imgs/' + img)\n",
    "    if pimg:\n",
    "        pimg.set_data(img)\n",
    "    else:\n",
    "        pimg = plt.imshow(img)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Questions (10 pts)\n",
    "***\n",
    "\n",
    "These are open-ended questions. The purpose is to encourage you to think (a bit) more deeply about these algorithms. You get full points as long as you write a few sentences that make sense and show some thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1 (2 pts): Why would people want to do function approximation rather than using tabular algorithm (on discretized S,A spaces if necessary)? Bringing function approximation has caused numerous problems theoretically (e.g. not guaranteed to converge), so it seems not worth it..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. People enjoy \"neuralizing\" things I guess.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2 (2 pts): Q-Learning seems good... it's theoretically sound (at least seems to be), the performance is also good. Why would many people actually prefer policy gradient type algorithms in some practical problems?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. I like Q learning. The name is cute. Anyone watch StarTrek?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3 (2 pts): Does the policy gradient algorithm (A2C) we implemented here extend to continuous action space? How would you do that? Hint: What is a reasonable distribution assumption for policy $\\pi_{\\theta}(a|s)$ if $a$ lives in continuous space?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Maybe normalizing flow?? OK, people really do this..(arXiv:1905.06893) Hot area + hot area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4 (2 pts): The policy gradient algorithm (A2C) we implemented uses on-policy data. Can you think of a way to extend it to utilize off-policy data? Hint: Importance sampling, needs some approximation though"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Do random math tricks or pray?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5 (2 pts): How to compare different RL algorithms? When can I say one algorithm is better than the other? Hint: This question is quite open. Think about speed, complexity, tasks, etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer: I don't know. Just pick one you like, they're equally bad.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
